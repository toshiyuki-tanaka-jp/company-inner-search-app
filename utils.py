"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
from typing import List
from dotenv import load_dotenv
import streamlit as st
import urllib.parse
from openai import OpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage, Document
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import constants as ct

client = OpenAI()


############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_source_icon(source):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å–å¾—

    Args:
        source: å‚ç…§å…ƒã®ã‚ã‚Šã‹

    Returns:
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡
    """
    # å‚ç…§å…ƒãŒWebãƒšãƒ¼ã‚¸ã®å ´åˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã§ã€å–å¾—ã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹
    if source.startswith("http"):
        icon = ct.LINK_SOURCE_ICON
    else:
        icon = ct.DOC_SOURCE_ICON
    
    return icon


def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def build_pdf_view_url(source_path: str) -> str:
    """GitHubã®Raw URLã‚’Google Docs Viewerã§åŸ‹ã‚è¾¼ã¿/åˆ¥ã‚¿ãƒ–è¡¨ç¤ºã§ãã‚‹URLã«å¤‰æ›"""
    owner  = st.secrets.get("GITHUB_OWNER")
    repo   = st.secrets.get("GITHUB_REPO")
    branch = st.secrets.get("GITHUB_BRANCH", "main")
    rel = source_path.lstrip("./")
    raw = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{rel}"
    return f"https://drive.google.com/viewer?embedded=1&url={urllib.parse.quote(raw, safe='')}"


def render_evidence(docs, title="æƒ…å ±æº"):
    """é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆPDFã¯ãƒšãƒ¼ã‚¸ç•ªå·ä»˜ãï¼‰ã‚’ä¸€è¦§è¡¨ç¤º"""
    if not docs:
        return
    st.markdown(f"### {title}")
    for i, d in enumerate(docs):
        src = d.metadata.get("source", "unknown")
        page = d.metadata.get("page")
        page_str = f"ï¼ˆãƒšãƒ¼ã‚¸No.{page+1}ï¼‰" if isinstance(page, int) else ""
        if src.lower().endswith(".pdf"):
            url = build_pdf_view_url(src)
            # å…ˆé ­å€™è£œã¯æ·¡ã„ç·‘ã€ãã®ã»ã‹ã¯æ·¡ã„é’ã®ãƒãƒƒã‚¸é¢¨
            bg = "#e8f5e9" if i == 0 else "#e9f2ff"
            st.markdown(
                f"""
                <div style="background:{bg};padding:12px 16px;border-radius:12px;margin:8px 0;">
                  ğŸ“„ <a href="{url}" target="_blank">{src}</a> {page_str}
                </div>
                """,
                unsafe_allow_html=True,
            )
        else:
            st.markdown(
                f"""
                <div style="background:#e9f2ff;padding:12px 16px;border-radius:12px;margin:8px 0;">
                  ğŸ“„ {src}
                </div>
                """,
                unsafe_allow_html=True,
            )


def get_llm_response(prompt: str, docs: list, mode: str = "search") -> str:
    """LLMã«è¦ç´„ãƒ»å›ç­”ã‚’ç”Ÿæˆã•ã›ã‚‹"""
    context = "\n\n".join(
        f"Source: {d.metadata.get('source')} (page {d.metadata.get('page', 'N/A')})\n{d.page_content}"
        for d in docs
    )

    system_msg = "ã‚ãªãŸã¯ç¤¾å†…æ–‡æ›¸ã‚’ã‚‚ã¨ã«è³ªå•ã«ç­”ãˆã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
    user_msg = f"è³ªå•:\n{prompt}\n\nå‚è€ƒæƒ…å ±:\n{context}"

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=0.3,
    )

    return completion.choices[0].message.content.strip()


def get_llm_response_legacy(chat_message):
    """
    LLMã‹ã‚‰ã®å›ç­”å–å¾—ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ç‰ˆãƒ»ä¼šè©±å±¥æ­´å¯¾å¿œï¼‰

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”ï¼ˆè¾æ›¸å½¢å¼ï¼‰
    """
    # LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
    llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
    if st.session_state.mode == ct.ANSWER_MODE_1:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…æ–‡æ›¸æ¤œç´¢ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_DOC_SEARCH
    else:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…å•ã„åˆã‚ã›ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
    history_aware_retriever = create_history_aware_retriever(
        llm, st.session_state.retriever, question_generator_prompt
    )

    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    # ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—
    llm_response = chain.invoke({"input": chat_message, "chat_history": st.session_state.chat_history})
    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=chat_message), llm_response["answer"]])

    return llm_response


def get_relevant_docs(vectorstore, query, top_k=ct.RETRIEVAL_TOP_K):
    """
    ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
    
    Args:
        vectorstore: ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        top_k: å–å¾—ã™ã‚‹æ–‡æ›¸æ•°
        
    Returns:
        é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    try:
        retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
        relevant_docs = retriever.get_relevant_documents(query)
        return relevant_docs
    except Exception as e:
        st.error(f"æ–‡æ›¸æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return []


def get_llm_response_simple(user_input, relevant_docs):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªLLMå¿œç­”å–å¾—é–¢æ•°
    
    Args:
        user_input: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
        relevant_docs: é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        
    Returns:
        LLMå¿œç­”
    """
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    
    # LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
    llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)
    
    # æ–‡è„ˆã‚’ä½œæˆ
    context = "\n".join([doc.page_content for doc in relevant_docs])
    
    # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠ
    mode = st.session_state.get("mode", ct.ANSWER_MODE_1)
    if mode == ct.ANSWER_MODE_1:
        template = ct.SYSTEM_PROMPT_DOC_SEARCH
    else:
        template = ct.SYSTEM_PROMPT_INQUIRY
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    prompt = ChatPromptTemplate.from_template(
        template + "\n\nå…¥åŠ›: {input}"
    )
    
    # ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã—ã¦å®Ÿè¡Œ
    chain = prompt | llm
    response = chain.invoke({"input": user_input, "context": context})
    
    return {"answer": response.content}